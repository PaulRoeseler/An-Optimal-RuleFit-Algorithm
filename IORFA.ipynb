{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a52a15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# author: Bo Tang\n",
    "\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "\n",
    "def convert_dataframe_to_matrix(data):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        return data.to_numpy()\n",
    "    elif isinstance(data, pd.Series):\n",
    "        return data.to_numpy()\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "class optimalDecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    optimal classification tree\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=3, min_samples_split=2, alpha=0, warmstart=True, timelimit=600, output=True):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.alpha = alpha\n",
    "        self.warmstart = warmstart\n",
    "        self.timelimit = timelimit\n",
    "        self.output = output\n",
    "        self.trained = False\n",
    "        self.optgap = None\n",
    "\n",
    "        # scaler params (fit-time)\n",
    "        self.x_min_ = None\n",
    "        self.x_scale_ = None  # range, with zeros replaced by 1\n",
    "        self.scales = None    # kept for backward-compat (but now means x_scale_)\n",
    "\n",
    "        # node index\n",
    "        self.n_index = [i + 1 for i in range(2 ** (self.max_depth + 1) - 1)]\n",
    "        self.b_index = self.n_index[:-2 ** self.max_depth]  # branch nodes\n",
    "        self.l_index = self.n_index[-2 ** self.max_depth:]  # leaf nodes\n",
    "\n",
    "    def _ensure_2d_float(self, x):\n",
    "        x = convert_dataframe_to_matrix(x)\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        return x\n",
    "\n",
    "    def _fit_scaler(self, x):\n",
    "        \"\"\"Fit min-max scaler to [0,1] per feature.\"\"\"\n",
    "        x = self._ensure_2d_float(x)\n",
    "        self.x_min_ = np.min(x, axis=0)\n",
    "        x_max = np.max(x, axis=0)\n",
    "        self.x_scale_ = x_max - self.x_min_\n",
    "        # avoid division by zero for constant features\n",
    "        self.x_scale_[self.x_scale_ == 0] = 1.0\n",
    "        self.scales = self.x_scale_\n",
    "        return x\n",
    "\n",
    "    def _transform(self, x):\n",
    "        \"\"\"Apply fitted min-max scaling.\"\"\"\n",
    "        if self.x_min_ is None or self.x_scale_ is None:\n",
    "            raise AssertionError(\"Scaler is not fitted. Call fit() first.\")\n",
    "        x = self._ensure_2d_float(x)\n",
    "        if hasattr(self, \"p\") and x.shape[1] != self.p:\n",
    "            raise ValueError(f\"Expected {self.p} features, got {x.shape[1]}.\")\n",
    "        return (x - self.x_min_) / self.x_scale_\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        fit training data\n",
    "        \"\"\"\n",
    "        # convert data\n",
    "        x_raw = self._fit_scaler(x)\n",
    "        y = convert_dataframe_to_matrix(y)\n",
    "        y = np.asarray(y).ravel()\n",
    "\n",
    "        # scale data to [0,1] for MIP correctness\n",
    "        x_scaled = (x_raw - self.x_min_) / self.x_scale_\n",
    "\n",
    "        # data size\n",
    "        self.n, self.p = x_scaled.shape\n",
    "        if self.output:\n",
    "            print(f\"Training data include {self.n} instances, {self.p} features.\")\n",
    "\n",
    "        # labels\n",
    "        self.labels = np.unique(y)\n",
    "\n",
    "        # solve MIP (IMPORTANT: build on scaled data)\n",
    "        m, a, b, c, d, l = self._buildMIP(x_scaled, y)\n",
    "        if self.warmstart:\n",
    "            self._setStart(x_scaled, y, a, c, d, l)  # warmstart consistent with scaling\n",
    "        m.optimize()\n",
    "        self.optgap = m.MIPGap\n",
    "\n",
    "        # get parameters\n",
    "        self._a = {ind: a[ind].x for ind in a}\n",
    "        self._b = {ind: b[ind].x for ind in b}\n",
    "        self._c = {ind: c[ind].x for ind in c}\n",
    "        self._d = {ind: d[ind].x for ind in d}\n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        model prediction\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            raise AssertionError(\"This optimalDecisionTreeClassifier instance is not fitted yet.\")\n",
    "\n",
    "        # scale input (same transform as training)\n",
    "        x_scaled = self._transform(x)\n",
    "\n",
    "        # leaf label\n",
    "        labelmap = {}\n",
    "        for t in self.l_index:\n",
    "            # pick the label with the largest c-value (more robust than thresholding)\n",
    "            best_k = None\n",
    "            best_val = -1.0\n",
    "            for k in self.labels:\n",
    "                val = self._c[k, t]\n",
    "                if val > best_val:\n",
    "                    best_val = val\n",
    "                    best_k = k\n",
    "            if best_k is not None and best_val > 1e-6:\n",
    "                labelmap[t] = best_k\n",
    "\n",
    "        y_pred = []\n",
    "        for xi in x_scaled:\n",
    "            t = 1\n",
    "            while t not in self.l_index:\n",
    "                right = (sum(self._a[j, t] * xi[j] for j in range(self.p)) + 1e-9 >= self._b[t])\n",
    "                t = 2 * t + 1 if right else 2 * t\n",
    "            y_pred.append(labelmap.get(t, self.labels[0]))  # safe fallback\n",
    "\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _buildMIP(self, x, y):\n",
    "        \"\"\"\n",
    "        build MIP formulation for Optimal Decision Tree\n",
    "        NOTE: x is assumed scaled to [0,1].\n",
    "        \"\"\"\n",
    "        # create a model\n",
    "        m = gp.Model('m')\n",
    "\n",
    "        # output\n",
    "        m.Params.outputFlag = self.output\n",
    "        m.Params.LogToConsole = self.output\n",
    "        # time limit\n",
    "        m.Params.timelimit = self.timelimit\n",
    "        # parallel\n",
    "        m.params.threads = 0\n",
    "\n",
    "        # model sense\n",
    "        m.modelSense = GRB.MINIMIZE\n",
    "\n",
    "        # variables\n",
    "        a = m.addVars(self.p, self.b_index, vtype=GRB.BINARY, name='a')          # splitting feature\n",
    "        b = m.addVars(self.b_index, vtype=GRB.CONTINUOUS, name='b')              # splitting threshold (lb=0)\n",
    "        c = m.addVars(self.labels, self.l_index, vtype=GRB.BINARY, name='c')     # node prediction\n",
    "        d = m.addVars(self.b_index, vtype=GRB.BINARY, name='d')                  # splitting option\n",
    "        z = m.addVars(self.n, self.l_index, vtype=GRB.BINARY, name='z')          # leaf node assignment\n",
    "        l = m.addVars(self.l_index, vtype=GRB.BINARY, name='l')                  # leaf node activation\n",
    "        L = m.addVars(self.l_index, vtype=GRB.CONTINUOUS, name='L')              # leaf node misclassified\n",
    "        M = m.addVars(self.labels, self.l_index, vtype=GRB.CONTINUOUS, name='M') # leaf node samples with label\n",
    "        N = m.addVars(self.l_index, vtype=GRB.CONTINUOUS, name='N')              # leaf node samples\n",
    "\n",
    "        # calculate baseline accuracy\n",
    "        baseline = self._calBaseline(y)\n",
    "\n",
    "        # calculate minimum distance (on SCALED x)\n",
    "        min_dis = self._calMinDist(x)\n",
    "\n",
    "        # objective function\n",
    "        obj = L.sum() / baseline + self.alpha * d.sum()\n",
    "        m.setObjective(obj)\n",
    "\n",
    "        # constraints\n",
    "        # (20)\n",
    "        m.addConstrs(L[t] >= N[t] - M[k, t] - self.n * (1 - c[k, t])\n",
    "                     for t in self.l_index for k in self.labels)\n",
    "        # (21)\n",
    "        m.addConstrs(L[t] <= N[t] - M[k, t] + self.n * c[k, t]\n",
    "                     for t in self.l_index for k in self.labels)\n",
    "        # (17)\n",
    "        m.addConstrs(gp.quicksum((y[i] == k) * z[i, t] for i in range(self.n)) == M[k, t]\n",
    "                     for t in self.l_index for k in self.labels)\n",
    "        # (16)\n",
    "        m.addConstrs(z.sum('*', t) == N[t] for t in self.l_index)\n",
    "        # (18)\n",
    "        m.addConstrs(c.sum('*', t) == l[t] for t in self.l_index)\n",
    "\n",
    "        # (13) and (14)\n",
    "        bigM_left = 1.0 + float(np.max(min_dis))  # <= 2 if x is in [0,1]\n",
    "        for t in self.l_index:\n",
    "            left = (t % 2 == 0)\n",
    "            ta = t // 2\n",
    "            while ta != 0:\n",
    "                if left:\n",
    "                    m.addConstrs(\n",
    "                        gp.quicksum(a[j, ta] * (x[i, j] + min_dis[j]) for j in range(self.p))\n",
    "                        + bigM_left * (1 - d[ta])\n",
    "                        <= b[ta] + bigM_left * (1 - z[i, t])\n",
    "                        for i in range(self.n)\n",
    "                    )\n",
    "                else:\n",
    "                    # big-M = 1 is valid because x,b in [0,1]\n",
    "                    m.addConstrs(\n",
    "                        gp.quicksum(a[j, ta] * x[i, j] for j in range(self.p))\n",
    "                        >= b[ta] - (1 - z[i, t])\n",
    "                        for i in range(self.n)\n",
    "                    )\n",
    "                left = (ta % 2 == 0)\n",
    "                ta //= 2\n",
    "\n",
    "        # (8)\n",
    "        m.addConstrs(z.sum(i, '*') == 1 for i in range(self.n))\n",
    "        # (6)\n",
    "        m.addConstrs(z[i, t] <= l[t] for t in self.l_index for i in range(self.n))\n",
    "        # (7)\n",
    "        m.addConstrs(z.sum('*', t) >= self.min_samples_split * l[t] for t in self.l_index)\n",
    "        # (2)\n",
    "        m.addConstrs(a.sum('*', t) == d[t] for t in self.b_index)\n",
    "        # (3)  -> forces b in [0,1] when d=1 (since b has lb=0)\n",
    "        m.addConstrs(b[t] <= d[t] for t in self.b_index)\n",
    "        # (5)\n",
    "        m.addConstrs(d[t] <= d[t // 2] for t in self.b_index if t != 1)\n",
    "\n",
    "        return m, a, b, c, d, l\n",
    "\n",
    "    @staticmethod\n",
    "    def _calBaseline(y):\n",
    "        \"\"\"\n",
    "        obtain baseline accuracy by simply predicting the most popular class\n",
    "        \"\"\"\n",
    "        # robust mode extraction across scipy versions\n",
    "        res = stats.mode(y, keepdims=True)\n",
    "        mode = res.mode[0]\n",
    "        return np.sum(y == mode)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calMinDist(x):\n",
    "        \"\"\"\n",
    "        get the smallest positive distance per feature\n",
    "        x is assumed numeric; if scaled to [0,1], distances are <= 1.\n",
    "        \"\"\"\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        min_dis = []\n",
    "        for j in range(x.shape[1]):\n",
    "            xj = np.unique(x[:, j])\n",
    "            xj.sort()\n",
    "            diffs = np.diff(xj)\n",
    "            diffs = diffs[diffs > 0]\n",
    "            min_dis.append(float(diffs.min()) if diffs.size else 1.0)\n",
    "        return min_dis\n",
    "\n",
    "    def _setStart(self, x, y, a, c, d, l):\n",
    "        \"\"\"\n",
    "        set warm start from CART\n",
    "        NOTE: x should be scaled consistently with the MIP.\n",
    "        \"\"\"\n",
    "        if self.min_samples_split > 1:\n",
    "            clf = tree.DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "        else:\n",
    "            clf = tree.DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "        clf.fit(x, y)\n",
    "\n",
    "        # get splitting rules\n",
    "        rules = self._getRules(clf)\n",
    "\n",
    "        # fix branch node\n",
    "        for t in self.b_index:\n",
    "            if rules[t].feat is None or rules[t].feat == tree._tree.TREE_UNDEFINED:\n",
    "                d[t].start = 0\n",
    "                for f in range(self.p):\n",
    "                    a[f, t].start = 0\n",
    "            else:\n",
    "                d[t].start = 1\n",
    "                for f in range(self.p):\n",
    "                    a[f, t].start = 1 if f == int(rules[t].feat) else 0\n",
    "\n",
    "        # fix leaf nodes\n",
    "        for t in self.l_index:\n",
    "            if rules[t].value is None:\n",
    "                l[t].start = int(t % 2)\n",
    "                if t % 2:\n",
    "                    t_leaf = t\n",
    "                    while rules[t].value is None:\n",
    "                        t //= 2\n",
    "                    for k in self.labels:\n",
    "                        c[k, t_leaf].start = 1 if k == np.argmax(rules[t].value) else 0\n",
    "                else:\n",
    "                    for k in self.labels:\n",
    "                        c[k, t].start = 0\n",
    "            else:\n",
    "                l[t].start = 1\n",
    "                for k in self.labels:\n",
    "                    c[k, t].start = 1 if k == np.argmax(rules[t].value) else 0\n",
    "\n",
    "    def _getRules(self, clf):\n",
    "        \"\"\"\n",
    "        get splitting rules\n",
    "        \"\"\"\n",
    "        node_map = {1: 0}\n",
    "        for t in self.b_index:\n",
    "            node_map[2 * t] = -1\n",
    "            node_map[2 * t + 1] = -1\n",
    "            lch = clf.tree_.children_left[node_map[t]]\n",
    "            node_map[2 * t] = lch\n",
    "            rch = clf.tree_.children_right[node_map[t]]\n",
    "            node_map[2 * t + 1] = rch\n",
    "\n",
    "        rule = namedtuple('Rules', ('feat', 'threshold', 'value'))\n",
    "        rules = {}\n",
    "        for t in self.b_index:\n",
    "            i = node_map[t]\n",
    "            rules[t] = rule(None, None, None) if i == -1 else rule(clf.tree_.feature[i], clf.tree_.threshold[i], clf.tree_.value[i, 0])\n",
    "        for t in self.l_index:\n",
    "            i = node_map[t]\n",
    "            rules[t] = rule(None, None, None) if i == -1 else rule(None, None, clf.tree_.value[i, 0])\n",
    "        return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dbf6b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train acc: 0.950, Test acc: 0.903\n",
      "[1b] b in [0,1] on split nodes: OK\n",
      "[2] Test acc after harsh affine feature transform: 0.910\n",
      "[3] Fixed-solution feasibility: OK\n",
      "[3b] Leaf statistics consistency (N/M/L): OK\n",
      "[4] min_samples_split enforced: OK\n",
      "[5] splits alpha=0.0: 7, splits alpha=1.0: 0\n",
      "[5b] alpha regularization effect: OK\n",
      "[6] Warmstart OFF test acc: 0.933\n",
      "\n",
      "ALL FULL INTEGRATION TESTS PASSED ✅\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# ---- Adjust this import to your file/module name ----\n",
    "# from your_module import optimalDecisionTreeClassifier\n",
    "# -----------------------------------------------------\n",
    "\n",
    "\n",
    "def make_multiclass_data(n, seed=0):\n",
    "    \"\"\"\n",
    "    Depth-3-ish ground truth with:\n",
    "      - continuous + discrete + correlated + constant features\n",
    "      - 3 classes\n",
    "      - a small region that challenges min_samples_split\n",
    "      - label noise so misclassification variables matter\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    x0 = rng.random(n)                                # [0,1]\n",
    "    x1 = rng.random(n)                                # [0,1]\n",
    "    x2 = rng.choice([0.0, 0.5, 1.0], size=n, p=[0.55, 0.30, 0.15])  # discrete\n",
    "    x3 = rng.random(n)                                # noise feature\n",
    "    x4 = np.ones(n) * 5.0                             # constant column (tests scaling)\n",
    "    x5 = np.clip(0.35 * x0 + 0.65 * x1 + rng.normal(0, 0.03, n), 0, 1)  # correlated\n",
    "\n",
    "    X = np.vstack([x0, x1, x2, x3, x4, x5]).T\n",
    "\n",
    "    # Ground-truth decision logic (3 classes)\n",
    "    y = np.zeros(n, dtype=int)\n",
    "    y[x0 > 0.75] = 2\n",
    "    mask = (x0 <= 0.75) & (x1 > 0.60)\n",
    "    y[mask] = 1\n",
    "    mask = (x0 <= 0.75) & (x1 <= 0.60) & (x2 == 1.0)\n",
    "    y[mask] = 1\n",
    "\n",
    "    # Add some label noise (forces L/M/N to matter)\n",
    "    flip = rng.random(n) < 0.08\n",
    "    y[flip] = rng.integers(0, 3, size=flip.sum())\n",
    "\n",
    "    cols = [\"x0\", \"x1\", \"x2\", \"x3\", \"x4_const\", \"x5_corr\"]\n",
    "    X_df = pd.DataFrame(X, columns=cols)\n",
    "    y_sr = pd.Series(y, name=\"y\")\n",
    "    return X_df, y_sr\n",
    "\n",
    "\n",
    "def affine_transform(X_df):\n",
    "    \"\"\"Apply harsh affine transforms to test scaling invariance.\"\"\"\n",
    "    X2 = X_df.copy()\n",
    "    # huge scale + offset\n",
    "    X2[\"x0\"] = X2[\"x0\"] * 1e6 + 1234.0\n",
    "    # tiny scale + negative offset\n",
    "    X2[\"x1\"] = X2[\"x1\"] * 1e-4 - 9.0\n",
    "    # discrete with offset\n",
    "    X2[\"x2\"] = X2[\"x2\"] * 10.0 + 7.0\n",
    "    # noise with negative scaling\n",
    "    X2[\"x3\"] = -3.0 * X2[\"x3\"] + 2.0\n",
    "    # constant remains constant (still tests zero-range)\n",
    "    # correlated with scaling\n",
    "    X2[\"x5_corr\"] = X2[\"x5_corr\"] * 100.0 - 50.0\n",
    "    return X2\n",
    "\n",
    "\n",
    "def build_mip_all(clf, x_scaled, y):\n",
    "    \"\"\"\n",
    "    Rebuild the *full* MIP (including z, l, L, M, N) so we can:\n",
    "      - fix learned a,b,c,d\n",
    "      - solve for remaining vars\n",
    "      - validate feasibility + implied leaf stats\n",
    "    This mirrors your formulation (same constraints/indices).\n",
    "    \"\"\"\n",
    "    x_scaled = np.asarray(x_scaled, dtype=float)\n",
    "    y = np.asarray(y).ravel()\n",
    "\n",
    "    n, p = x_scaled.shape\n",
    "    labels = np.unique(y)\n",
    "\n",
    "    m = gp.Model(\"check_mip\")\n",
    "    m.Params.OutputFlag = 0\n",
    "    m.Params.LogToConsole = 0\n",
    "    m.Params.TimeLimit = 30\n",
    "    m.Params.Threads = 0\n",
    "    m.ModelSense = GRB.MINIMIZE\n",
    "\n",
    "    a = m.addVars(p, clf.b_index, vtype=GRB.BINARY, name=\"a\")\n",
    "    b = m.addVars(clf.b_index, vtype=GRB.CONTINUOUS, name=\"b\")\n",
    "    c = m.addVars(labels, clf.l_index, vtype=GRB.BINARY, name=\"c\")\n",
    "    d = m.addVars(clf.b_index, vtype=GRB.BINARY, name=\"d\")\n",
    "    z = m.addVars(n, clf.l_index, vtype=GRB.BINARY, name=\"z\")\n",
    "    l = m.addVars(clf.l_index, vtype=GRB.BINARY, name=\"l\")\n",
    "    L = m.addVars(clf.l_index, vtype=GRB.CONTINUOUS, name=\"L\")\n",
    "    M = m.addVars(labels, clf.l_index, vtype=GRB.CONTINUOUS, name=\"M\")\n",
    "    N = m.addVars(clf.l_index, vtype=GRB.CONTINUOUS, name=\"N\")\n",
    "\n",
    "    baseline = clf._calBaseline(y)\n",
    "    min_dis = clf._calMinDist(x_scaled)\n",
    "    bigM_left = 1.0 + float(np.max(min_dis))\n",
    "\n",
    "    # objective (same structure)\n",
    "    m.setObjective(L.sum() / baseline + clf.alpha * d.sum())\n",
    "\n",
    "    # (20) (21)\n",
    "    m.addConstrs(L[t] >= N[t] - M[k, t] - n * (1 - c[k, t])\n",
    "                 for t in clf.l_index for k in labels)\n",
    "    m.addConstrs(L[t] <= N[t] - M[k, t] + n * c[k, t]\n",
    "                 for t in clf.l_index for k in labels)\n",
    "\n",
    "    # (17) M counts\n",
    "    m.addConstrs(gp.quicksum((y[i] == k) * z[i, t] for i in range(n)) == M[k, t]\n",
    "                 for t in clf.l_index for k in labels)\n",
    "    # (16) N counts\n",
    "    m.addConstrs(z.sum(\"*\", t) == N[t] for t in clf.l_index)\n",
    "    # (18) one label per active leaf\n",
    "    m.addConstrs(c.sum(\"*\", t) == l[t] for t in clf.l_index)\n",
    "\n",
    "    # (13) (14) path constraints\n",
    "    for t in clf.l_index:\n",
    "        left = (t % 2 == 0)\n",
    "        ta = t // 2\n",
    "        while ta != 0:\n",
    "            if left:\n",
    "                m.addConstrs(\n",
    "                    gp.quicksum(a[j, ta] * (x_scaled[i, j] + min_dis[j]) for j in range(p))\n",
    "                    + bigM_left * (1 - d[ta])\n",
    "                    <= b[ta] + bigM_left * (1 - z[i, t])\n",
    "                    for i in range(n)\n",
    "                )\n",
    "            else:\n",
    "                m.addConstrs(\n",
    "                    gp.quicksum(a[j, ta] * x_scaled[i, j] for j in range(p))\n",
    "                    >= b[ta] - (1 - z[i, t])\n",
    "                    for i in range(n)\n",
    "                )\n",
    "            left = (ta % 2 == 0)\n",
    "            ta //= 2\n",
    "\n",
    "    # (8) each sample assigned exactly one leaf\n",
    "    m.addConstrs(z.sum(i, \"*\") == 1 for i in range(n))\n",
    "    # (6) assignment implies leaf active\n",
    "    m.addConstrs(z[i, t] <= l[t] for t in clf.l_index for i in range(n))\n",
    "    # (7) min samples per active leaf\n",
    "    m.addConstrs(z.sum(\"*\", t) >= clf.min_samples_split * l[t] for t in clf.l_index)\n",
    "\n",
    "    # (2) one feature if split\n",
    "    m.addConstrs(a.sum(\"*\", t) == d[t] for t in clf.b_index)\n",
    "    # (3) b <= d (with b>=0 default) => b in [0,1] if split\n",
    "    m.addConstrs(b[t] <= d[t] for t in clf.b_index)\n",
    "    # (5) parent split if child split\n",
    "    m.addConstrs(d[t] <= d[t // 2] for t in clf.b_index if t != 1)\n",
    "\n",
    "    return m, a, b, c, d, z, l, L, M, N, labels\n",
    "\n",
    "\n",
    "def fix_solution_in_mip(m, a, b, c, d, clf, labels, eps_b=1e-6):\n",
    "    \"\"\"Fix learned (a,b,c,d) in the rebuilt MIP (with tolerance on b).\"\"\"\n",
    "    # Fix d and a\n",
    "    for t in clf.b_index:\n",
    "        dt = int(round(clf._d[t]))\n",
    "        m.addConstr(d[t] == dt)\n",
    "        for j in range(clf.p):\n",
    "            ajt = int(round(clf._a[j, t]))\n",
    "            m.addConstr(a[j, t] == ajt)\n",
    "\n",
    "        bt = float(clf._b[t])\n",
    "        # tolerate tiny numerical differences\n",
    "        m.addConstr(b[t] >= bt - eps_b)\n",
    "        m.addConstr(b[t] <= bt + eps_b)\n",
    "\n",
    "    # Fix c\n",
    "    for t in clf.l_index:\n",
    "        for k in labels:\n",
    "            ckt = int(round(clf._c[k, t]))\n",
    "            m.addConstr(c[k, t] == ckt)\n",
    "\n",
    "\n",
    "def assert_close(a, b, tol, msg):\n",
    "    if abs(a - b) > tol:\n",
    "        raise AssertionError(f\"{msg}: {a} vs {b} (tol={tol})\")\n",
    "\n",
    "\n",
    "def run_full_test_suite():\n",
    "    # --------------------------\n",
    "    # 1) Train + Predict + Types\n",
    "    # --------------------------\n",
    "    X_train, y_train = make_multiclass_data(180, seed=1)\n",
    "    X_test, y_test = make_multiclass_data(300, seed=2)\n",
    "\n",
    "    clf = optimalDecisionTreeClassifier(\n",
    "        max_depth=3,\n",
    "        min_samples_split=2,\n",
    "        alpha=0.0,\n",
    "        warmstart=True,\n",
    "        timelimit=60,\n",
    "        output=False,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # scaling sanity: transform is [0,1] (constant columns become 0 after min-max)\n",
    "    Xs = clf._transform(X_train)\n",
    "    if not (np.nanmin(Xs) >= -1e-9 and np.nanmax(Xs) <= 1.0 + 1e-9):\n",
    "        raise AssertionError(\"Scaled features are not within [0,1]. Scaling is inconsistent.\")\n",
    "\n",
    "    # accuracy sanity on noisy multiclass\n",
    "    yhat_train = clf.predict(X_train)\n",
    "    yhat_test = clf.predict(X_test)\n",
    "    acc_tr = (yhat_train == np.asarray(y_train)).mean()\n",
    "    acc_te = (yhat_test == np.asarray(y_test)).mean()\n",
    "    print(f\"[1] Train acc: {acc_tr:.3f}, Test acc: {acc_te:.3f}\")\n",
    "    if acc_tr < 0.85 or acc_te < 0.82:\n",
    "        raise AssertionError(\"Accuracy unexpectedly low (could indicate scaling/constraints issue).\")\n",
    "\n",
    "    # b bounds when split\n",
    "    for t in clf.b_index:\n",
    "        if int(round(clf._d[t])) == 1:\n",
    "            bt = float(clf._b[t])\n",
    "            if bt < -1e-6 or bt > 1.0 + 1e-6:\n",
    "                raise AssertionError(f\"Threshold b[{t}] out of [0,1] while split: {bt}\")\n",
    "    print(\"[1b] b in [0,1] on split nodes: OK\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # 2) Affine Transform Invariance (scaling robustness)\n",
    "    # -------------------------------------------------\n",
    "    X_train_aff = affine_transform(X_train)\n",
    "    X_test_aff = affine_transform(X_test)\n",
    "\n",
    "    clf_aff = optimalDecisionTreeClassifier(\n",
    "        max_depth=3,\n",
    "        min_samples_split=2,\n",
    "        alpha=0.0,\n",
    "        warmstart=True,\n",
    "        timelimit=60,\n",
    "        output=False,\n",
    "    )\n",
    "    clf_aff.fit(X_train_aff, y_train)\n",
    "    yhat_test_aff = clf_aff.predict(X_test_aff)\n",
    "    acc_aff = (yhat_test_aff == np.asarray(y_test)).mean()\n",
    "    print(f\"[2] Test acc after harsh affine feature transform: {acc_aff:.3f}\")\n",
    "    if acc_aff < 0.82:\n",
    "        raise AssertionError(\"Accuracy dropped after affine transform; scaling likely inconsistent.\")\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3) Full Constraint Consistency Check (rebuild + fix a,b,c,d)\n",
    "    # ---------------------------------------------------------\n",
    "    Xs_train = clf._transform(X_train)\n",
    "    m, a, b, c, d, z, l, L, M, N, labels = build_mip_all(clf, Xs_train, np.asarray(y_train))\n",
    "    fix_solution_in_mip(m, a, b, c, d, clf, labels, eps_b=1e-5)\n",
    "\n",
    "    m.optimize()\n",
    "    if m.SolCount == 0:\n",
    "        raise AssertionError(\"Fixed-solution feasibility check failed: no feasible solution found.\")\n",
    "    print(\"[3] Fixed-solution feasibility: OK\")\n",
    "\n",
    "    # Verify core implied logic from the solved (z,l,N,M,L)\n",
    "    # - each i assigned to exactly one leaf (already constrained, but we check numerically)\n",
    "    zvals = {(i, t): z[i, t].X for i in range(clf.n) for t in clf.l_index}\n",
    "    for i in range(clf.n):\n",
    "        s = sum(zvals[i, t] for t in clf.l_index)\n",
    "        assert_close(s, 1.0, 1e-5, f\"z-sum for sample {i}\")\n",
    "\n",
    "    # - min_samples_split for active leaves\n",
    "    for t in clf.l_index:\n",
    "        lt = l[t].X\n",
    "        Nt = N[t].X\n",
    "        if lt > 0.5:\n",
    "            if Nt + 1e-6 < clf.min_samples_split:\n",
    "                raise AssertionError(f\"Active leaf {t} violates min_samples_split: N={Nt}\")\n",
    "\n",
    "    # - L equals misclassified count at leaf (at optimum it should match N - max_k M)\n",
    "    for t in clf.l_index:\n",
    "        Nt = N[t].X\n",
    "        maxM = max(M[k, t].X for k in labels) if labels.size else 0.0\n",
    "        implied = Nt - maxM\n",
    "        if abs(L[t].X - implied) > 1e-4:\n",
    "            raise AssertionError(f\"Leaf {t} L mismatch: L={L[t].X} implied={implied}\")\n",
    "    print(\"[3b] Leaf statistics consistency (N/M/L): OK\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 4) min_samples_split stress (small region)\n",
    "    # --------------------------------------------\n",
    "    clf_ms = optimalDecisionTreeClassifier(\n",
    "        max_depth=3,\n",
    "        min_samples_split=25,  # this should prevent carving out the tiny x2==1 region\n",
    "        alpha=0.0,\n",
    "        warmstart=True,\n",
    "        timelimit=60,\n",
    "        output=False,\n",
    "    )\n",
    "    clf_ms.fit(X_train, y_train)\n",
    "\n",
    "    Xs_train_ms = clf_ms._transform(X_train)\n",
    "    m2, a2, b2, c2, d2, z2, l2, L2, M2, N2, labels2 = build_mip_all(clf_ms, Xs_train_ms, np.asarray(y_train))\n",
    "    fix_solution_in_mip(m2, a2, b2, c2, d2, clf_ms, labels2, eps_b=1e-5)\n",
    "    m2.optimize()\n",
    "    if m2.SolCount == 0:\n",
    "        raise AssertionError(\"min_samples_split feasibility check failed (fixed solution infeasible).\")\n",
    "\n",
    "    for t in clf_ms.l_index:\n",
    "        if l2[t].X > 0.5 and N2[t].X + 1e-6 < clf_ms.min_samples_split:\n",
    "            raise AssertionError(f\"min_samples_split violated at leaf {t}: N={N2[t].X}\")\n",
    "    print(\"[4] min_samples_split enforced: OK\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 5) alpha regularization tends to reduce splits\n",
    "    # --------------------------------------------\n",
    "    clf_lo = optimalDecisionTreeClassifier(\n",
    "        max_depth=3, min_samples_split=2, alpha=0.0, warmstart=True, timelimit=60, output=False\n",
    "    )\n",
    "    clf_hi = optimalDecisionTreeClassifier(\n",
    "        max_depth=3, min_samples_split=2, alpha=1.0, warmstart=True, timelimit=60, output=False\n",
    "    )\n",
    "    clf_lo.fit(X_train, y_train)\n",
    "    clf_hi.fit(X_train, y_train)\n",
    "\n",
    "    splits_lo = sum(int(round(clf_lo._d[t])) for t in clf_lo.b_index)\n",
    "    splits_hi = sum(int(round(clf_hi._d[t])) for t in clf_hi.b_index)\n",
    "    print(f\"[5] splits alpha=0.0: {splits_lo}, splits alpha=1.0: {splits_hi}\")\n",
    "    if splits_hi > splits_lo:\n",
    "        raise AssertionError(\"Higher alpha produced MORE splits (unexpected).\")\n",
    "    print(\"[5b] alpha regularization effect: OK\")\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # 6) Warmstart off should still work\n",
    "    # --------------------------------------------\n",
    "    clf_nows = optimalDecisionTreeClassifier(\n",
    "        max_depth=3, min_samples_split=2, alpha=0.0, warmstart=False, timelimit=60, output=False\n",
    "    )\n",
    "    clf_nows.fit(X_train, y_train)\n",
    "    acc_nows = (clf_nows.predict(X_test) == np.asarray(y_test)).mean()\n",
    "    print(f\"[6] Warmstart OFF test acc: {acc_nows:.3f}\")\n",
    "    if acc_nows < 0.80:\n",
    "        raise AssertionError(\"Warmstart=False produced unexpectedly low accuracy.\")\n",
    "    print(\"\\nALL FULL INTEGRATION TESTS PASSED ✅\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_test_suite()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "An-Optimal-RuleFit-Algorithm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
