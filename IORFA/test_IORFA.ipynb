{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d4e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# author: Bo Tang\n",
    "\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from sklearn import tree\n",
    "\n",
    "class optimalDecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Optimal regression tree with an integrated linear term (IORFA).\n",
    "    Prediction: f(x) = x @ beta + gamma_leaf.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_depth=3, min_samples_split=2, alpha=0, warmstart=True, timelimit=600, output=True,\n",
    "                 gamma_bounds=(-10000, 10000)):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.alpha = alpha\n",
    "        self.warmstart = warmstart\n",
    "        self.timelimit = timelimit\n",
    "        self.output = output\n",
    "        self.gamma_bounds = gamma_bounds\n",
    "        self.trained = False\n",
    "        self.optgap = None\n",
    "        self.feature_names_ = None\n",
    "        self.feature_mins_ = None\n",
    "        self.feature_maxs_ = None\n",
    "        self.big_m_ = None\n",
    "        self._beta = None\n",
    "        self._gamma = None\n",
    "\n",
    "        # node index\n",
    "        self.n_index = [i+1 for i in range(2 ** (self.max_depth + 1) - 1)]\n",
    "        self.b_index = self.n_index[:-2**self.max_depth] # branch nodes\n",
    "        self.l_index = self.n_index[-2**self.max_depth:] # leaf nodes\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        fit training data\n",
    "        \"\"\"\n",
    "        if hasattr(x, \"columns\"):\n",
    "            self.feature_names_ = list(x.columns)\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        y = np.asarray(y, dtype=float).ravel()\n",
    "\n",
    "        # data size\n",
    "        self.n, self.p = x.shape\n",
    "        self.feature_mins_ = np.min(x, axis=0)\n",
    "        self.feature_maxs_ = np.max(x, axis=0)\n",
    "        if self.output:\n",
    "            print('Training data include {} instances, {} features.'.format(self.n,self.p))\n",
    "\n",
    "        # solve MIP\n",
    "        self.m, self.a, self.b, self.d, self.l, self.beta, self.gamma = self._buildMIP(x, y)\n",
    "        \n",
    "        if self.warmstart:\n",
    "            self._setStart(x, y, self.a, self.b, self.d, self.l)\n",
    "\n",
    "        self.m.optimize()\n",
    "        self.optgap = self.m.MIPGap\n",
    "\n",
    "        # get parameters\n",
    "        self._a = {ind:self.a[ind].x for ind in self.a}\n",
    "        self._b = {ind:self.b[ind].x for ind in self.b}\n",
    "        self._d = {ind:self.d[ind].x for ind in self.d}\n",
    "        self._beta = np.array([self.beta[p].x for p in range(self.p)])\n",
    "        self._gamma = {t:self.gamma[t].x for t in self.l_index}\n",
    "\n",
    "        self.trained = True\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        model prediction\n",
    "        \"\"\"\n",
    "        if not self.trained:\n",
    "            raise AssertionError('This optimalDecisionTreeClassifier instance is not fitted yet.')\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1)\n",
    "        if x.shape[1] != self.p:\n",
    "            raise ValueError(f'Expected {self.p} features, got {x.shape[1]}.')\n",
    "\n",
    "        y_pred = np.zeros(x.shape[0], dtype=float)\n",
    "        for i, xi in enumerate(x):\n",
    "            t = 1\n",
    "            while t not in self.l_index:\n",
    "                if self._d.get(t, 0.0) < 0.5:\n",
    "                    t = 2 * t + 1\n",
    "                    continue\n",
    "                split_val = sum(self._a[j, t] * xi[j] for j in range(self.p))\n",
    "                if split_val + 1e-9 >= self._b[t]:\n",
    "                    t = 2 * t + 1\n",
    "                else:\n",
    "                    t = 2 * t\n",
    "            y_pred[i] = float(np.dot(xi, self._beta) + self._gamma[t])\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def _buildMIP(self, x, y):\n",
    "        \"\"\"\n",
    "        build MIP formulation for Optimal Decision Tree\n",
    "        \"\"\"\n",
    "        # create a model\n",
    "        m = gp.Model('m')\n",
    "\n",
    "        # output\n",
    "        m.Params.outputFlag = self.output\n",
    "        m.Params.LogToConsole = self.output\n",
    "        # time limit\n",
    "        m.Params.timelimit = self.timelimit\n",
    "        # parallel\n",
    "        m.params.threads = 0\n",
    "\n",
    "        # model sense\n",
    "        m.modelSense = GRB.MINIMIZE\n",
    "\n",
    "        # variables\n",
    "        a = m.addVars(self.p, self.b_index, vtype=GRB.BINARY, name='a') # splitting feature\n",
    "        b = m.addVars(self.b_index, vtype=GRB.CONTINUOUS, name='b') # splitting threshold\n",
    "        d = m.addVars(self.b_index, vtype=GRB.BINARY, name='d') # splitting option\n",
    "        z = m.addVars(self.n, self.l_index, vtype=GRB.BINARY, name='z') # leaf node assignment\n",
    "        l = m.addVars(self.l_index, vtype=GRB.BINARY, name='l') # leaf node activation\n",
    "        beta = m.addVars(self.p, vtype=GRB.CONTINUOUS, name='beta')\n",
    "        varkappa = m.addVars(self.n, self.l_index, vtype=GRB.CONTINUOUS, name='varkappa')\n",
    "        gamma = m.addVars(self.l_index, vtype=GRB.CONTINUOUS, name='gamma')\n",
    "        lamb = m.addVars(self.n, vtype=GRB.CONTINUOUS, name='f')\n",
    "        N = m.addVars(self.l_index, vtype=GRB.CONTINUOUS, name='N') # leaf node samples\n",
    "\n",
    "        # calculate minimum distance\n",
    "        min_dis = self._calMinDist(x)\n",
    "\n",
    "        feature_min = self.feature_mins_ if self.feature_mins_ is not None else np.min(x, axis=0)\n",
    "        feature_max = self.feature_maxs_ if self.feature_maxs_ is not None else np.max(x, axis=0)\n",
    "        feature_range = feature_max - feature_min\n",
    "        max_range = float(np.max(feature_range)) if feature_range.size else 0.0\n",
    "        eps_max = float(np.max(min_dis)) if len(min_dis) > 0 else 0.0\n",
    "        big_m = max_range + eps_max\n",
    "        if big_m <= 0:\n",
    "            big_m = 1.0\n",
    "        self.big_m_ = big_m\n",
    "\n",
    "        objExp = gp.QuadExpr()\n",
    "\n",
    "        self.Lower, self.Upper = self.gamma_bounds\n",
    "\n",
    "        # add single terms using add\n",
    "        for i in range(self.n):\n",
    "            var = y[i] - gp.quicksum(x[i, p] * beta[p] for p in range(self.p)) - lamb[i]\n",
    "\n",
    "            objExp.add(var * var) \n",
    "            \n",
    "            m.addConstr(lamb[i] == gp.quicksum(varkappa[i, t] for t in self.l_index))\n",
    "            \n",
    "            for t in self.l_index:\n",
    "                m.addConstr(self.Lower*z[i, t] <= varkappa[i, t])\n",
    "                m.addConstr(varkappa[i, t] <= self.Upper*z[i, t])\n",
    "                m.addConstr(self.Lower*(1-z[i, t]) <= gamma[t]-varkappa[i, t])\n",
    "                m.addConstr(gamma[t]-varkappa[i, t] <= self.Upper*(1-z[i, t]))\n",
    "                \n",
    "            # gp.quicksum(gamma[t]*z[i, t] for t in self.l_index))\n",
    "\n",
    "        complexity = gp.quicksum(d[t] for t in self.b_index)\n",
    "        m.setObjective((1.0 / self.n) * objExp + self.alpha * complexity)\n",
    "\n",
    "        # (16)\n",
    "        m.addConstrs(z.sum('*', t) == N[t] for t in self.l_index)\n",
    "        # (13) and (14)\n",
    "        for t in self.l_index:\n",
    "            left = (t % 2 == 0)\n",
    "            ta = t // 2\n",
    "            while ta != 0:\n",
    "                if left:\n",
    "                    m.addConstrs(gp.quicksum(a[j,ta] * (x[i,j] + min_dis[j]) for j in range(self.p))\n",
    "                                 +\n",
    "                                 big_m * (1 - d[ta])\n",
    "                                 <=\n",
    "                                 b[ta] + big_m * (1 - z[i,t])\n",
    "                                 for i in range(self.n))\n",
    "                else:\n",
    "                    m.addConstrs(gp.quicksum(a[j,ta] * x[i,j] for j in range(self.p))\n",
    "                                 >=\n",
    "                                 b[ta] - big_m * (1 - z[i,t])\n",
    "                                 for i in range(self.n))\n",
    "                left = (ta % 2 == 0)\n",
    "                ta //= 2\n",
    "\n",
    "        # (8)\n",
    "        m.addConstrs(z.sum(i, '*') == 1 for i in range(self.n))\n",
    "        # (6)\n",
    "        m.addConstrs(z[i,t] <= l[t] for t in self.l_index for i in range(self.n))\n",
    "        # (7)\n",
    "        m.addConstrs(z.sum('*', t) >= self.min_samples_split * l[t] for t in self.l_index)\n",
    "        # (2)\n",
    "        m.addConstrs(a.sum('*', t) == d[t] for t in self.b_index)\n",
    "        # (3)\n",
    "        m.addConstrs(b[t] <= gp.quicksum(feature_max[j] * a[j, t] for j in range(self.p)) for t in self.b_index)\n",
    "        m.addConstrs(b[t] >= gp.quicksum(feature_min[j] * a[j, t] for j in range(self.p)) for t in self.b_index)\n",
    "        # (5)\n",
    "        m.addConstrs(d[t] <= d[t//2] for t in self.b_index if t != 1)\n",
    "\n",
    "        return m, a, b, d, l, beta, gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def _calMinDist(x):\n",
    "        \"\"\"\n",
    "        get the smallest non-zero distance of features\n",
    "        \"\"\"\n",
    "        min_dis = []\n",
    "        for j in range(x.shape[1]):\n",
    "            xj = x[:,j]\n",
    "            # drop duplicates\n",
    "            xj = np.unique(xj)\n",
    "            # sort\n",
    "            xj = np.sort(xj)[::-1]\n",
    "            # distance\n",
    "            dis = [1]\n",
    "            for i in range(len(xj)-1):\n",
    "                dis.append(xj[i] - xj[i+1])\n",
    "            # min distance\n",
    "            min_dis.append(np.min(dis) if np.min(dis) else 1)\n",
    "        return min_dis\n",
    "\n",
    "    def _setStart(self, x, y, a, b, d, l):\n",
    "        \"\"\"\n",
    "        set warm start from CART\n",
    "        \"\"\"\n",
    "        # train with CART\n",
    "        if self.min_samples_split > 1:\n",
    "            clf = tree.DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "        else:\n",
    "            clf = tree.DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "        \n",
    "        clf.fit(x, y)\n",
    "\n",
    "        # get splitting rules\n",
    "        rules = self._getRules(clf)\n",
    "\n",
    "        # fix branch node\n",
    "        for t in self.b_index:\n",
    "            # not split\n",
    "            if rules[t].feat is None or rules[t].feat == tree._tree.TREE_UNDEFINED:\n",
    "                d[t].start = 0\n",
    "                b[t].start = 0\n",
    "                for f in range(self.p):\n",
    "                    a[f,t].start = 0\n",
    "            # split\n",
    "            else:\n",
    "                d[t].start = 1\n",
    "                b[t].start = rules[t].threshold\n",
    "                for f in range(self.p):\n",
    "                    if f == int(rules[t].feat):\n",
    "                        a[f,t].start = 1\n",
    "                    else:\n",
    "                        a[f,t].start = 0\n",
    "\n",
    "        # fix leaf nodes\n",
    "        for t in self.l_index:\n",
    "            # terminate early\n",
    "            if rules[t].value is None:\n",
    "                l[t].start = int(t % 2)\n",
    "                # flows go to right\n",
    "                # if t % 2:\n",
    "                #     t_leaf = t\n",
    "                #     while rules[t].value is None:\n",
    "                #         t //= 2\n",
    "                #     for k in self.labels:\n",
    "                #         if k == np.argmax(rules[t].value):\n",
    "                #             c[k, t_leaf].start = 1\n",
    "                #         else:\n",
    "                #             c[k, t_leaf].start = 0\n",
    "                # nothing in left\n",
    "                # else:\n",
    "                #     for k in self.labels:\n",
    "                #         c[k, t].start = 0\n",
    "            # terminate at leaf node\n",
    "            else:\n",
    "                l[t].start = 1\n",
    "                # for k in self.labels:\n",
    "                #     if k == np.argmax(rules[t].value):\n",
    "                #         c[k, t].start = 1\n",
    "                #     else:\n",
    "                #         c[k, t].start = 0\n",
    "\n",
    "    def _getRules(self, clf):\n",
    "        \"\"\"\n",
    "        get splitting rules\n",
    "        \"\"\"\n",
    "        # node index map\n",
    "        node_map = {1:0}\n",
    "        for t in self.b_index:\n",
    "            # terminal\n",
    "            node_map[2*t] = -1\n",
    "            node_map[2*t+1] = -1\n",
    "            if node_map[t] == -1:\n",
    "                continue\n",
    "            # left\n",
    "            l = clf.tree_.children_left[node_map[t]]\n",
    "            node_map[2*t] = l\n",
    "            # right\n",
    "            r = clf.tree_.children_right[node_map[t]]\n",
    "            node_map[2*t+1] = r\n",
    "\n",
    "        # rules\n",
    "        rule = namedtuple('Rules', ('feat', 'threshold', 'value'))\n",
    "        rules = {}\n",
    "        # branch nodes\n",
    "        for t in self.b_index:\n",
    "            i = node_map[t]\n",
    "            if i == -1:\n",
    "                r = rule(None, None, None)\n",
    "            else:\n",
    "                r = rule(clf.tree_.feature[i], clf.tree_.threshold[i], clf.tree_.value[i,0])\n",
    "            rules[t] = r\n",
    "        # leaf nodes\n",
    "        for t in self.l_index:\n",
    "            i = node_map[t]\n",
    "            if i == -1:\n",
    "                r = rule(None, None, None)\n",
    "            else:\n",
    "                r = rule(None, None, clf.tree_.value[i,0])\n",
    "            rules[t] = r\n",
    "\n",
    "        return rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae477cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== diabetes_sklearn ===\n",
      "LinearRegression RMSE=3338.7253  MAE=46.4920  R2=0.4069  fit_s=0.01\n",
      "Ridge(alpha=1.0) RMSE=3345.7090  MAE=46.6431  R2=0.4057  fit_s=0.01\n",
      "Lasso(alpha=0.01) RMSE=3340.1610  MAE=46.5259  R2=0.4067  fit_s=0.00\n",
      "CART             RMSE=7017.7748  MAE=66.6937  R2=-0.2466  fit_s=0.01\n",
      "RandomForest     RMSE=3459.4941  MAE=48.1021  R2=0.3855  fit_s=0.54\n",
      "ExtraTrees       RMSE=3456.6867  MAE=47.5510  R2=0.3860  fit_s=0.44\n",
      "GradientBoosting RMSE=3767.9006  MAE=50.3676  R2=0.3307  fit_s=0.10\n",
      "Set parameter WLSAccessID\n",
      "Set parameter WLSSecret\n",
      "Set parameter LicenseID to value 2755804\n",
      "Academic license 2755804 - for non-commercial use only - registered to pa___@mit.edu\n",
      "IORFA            RMSE=3527.9729  MAE=47.9889  R2=0.3733  fit_s=60.36  gap=0.9999930547747841  status=9\n",
      "\n",
      "=== california_housing_sklearn ===\n",
      "LinearRegression RMSE=0.4791  MAE=0.4996  R2=0.6407  fit_s=0.01\n",
      "Ridge(alpha=1.0) RMSE=0.4791  MAE=0.4996  R2=0.6407  fit_s=0.01\n",
      "Lasso(alpha=0.01) RMSE=0.4868  MAE=0.5018  R2=0.6349  fit_s=0.01\n",
      "CART             RMSE=0.6396  MAE=0.5420  R2=0.5203  fit_s=0.04\n",
      "RandomForest     RMSE=0.3124  MAE=0.3798  R2=0.7657  fit_s=1.30\n",
      "ExtraTrees       RMSE=0.2981  MAE=0.3655  R2=0.7764  fit_s=0.82\n",
      "GradientBoosting RMSE=0.2927  MAE=0.3828  R2=0.7805  fit_s=0.80\n",
      "IORFA            RMSE=0.6112  MAE=0.6122  R2=0.5530  fit_s=60.45  gap=0.9798188449327009  status=9\n",
      "\n",
      "=== abalone_183 ===\n",
      "SKIP (load failed): ImportError: Returning pandas objects requires pandas to be installed. Alternatively, explicitly set `as_frame=False` and `parser='liac-arff'`.\n",
      "\n",
      "=== wine_quality_287 ===\n",
      "SKIP (load failed): ImportError: Returning pandas objects requires pandas to be installed. Alternatively, explicitly set `as_frame=False` and `parser='liac-arff'`.\n",
      "\n",
      "=== kin8nm_189 ===\n",
      "SKIP (load failed): ImportError: Returning pandas objects requires pandas to be installed. Alternatively, explicitly set `as_frame=False` and `parser='liac-arff'`.\n",
      "\n",
      "=== cpu_act_197 ===\n",
      "SKIP (load failed): ImportError: Returning pandas objects requires pandas to be installed. Alternatively, explicitly set `as_frame=False` and `parser='liac-arff'`.\n",
      "\n",
      "=== cpu_small_562 ===\n",
      "SKIP (load failed): ImportError: Returning pandas objects requires pandas to be installed. Alternatively, explicitly set `as_frame=False` and `parser='liac-arff'`.\n",
      "\n",
      "=== yacht_hydrodynamics_42370 ===\n",
      "SKIP (load failed): ImportError: Returning pandas objects requires pandas to be installed. Alternatively, explicitly set `as_frame=False` and `parser='liac-arff'`.\n",
      "\n",
      "=== energy_efficiency_1472 ===\n",
      "SKIP (load failed): ImportError: Returning pandas objects requires pandas to be installed. Alternatively, explicitly set `as_frame=False` and `parser='liac-arff'`.\n",
      "\n",
      "=== concrete_strength_44959 ===\n",
      "SKIP (load failed): ImportError: Returning pandas objects requires pandas to be installed. Alternatively, explicitly set `as_frame=False` and `parser='liac-arff'`.\n",
      "\n",
      "Install pandas to save CSV nicely: pip install pandas\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# benchmark_iorfa_real.py\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Baselines + utilities ---\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes, fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetSpec:\n",
    "    name: str\n",
    "    loader: Callable[[int], Tuple[object, np.ndarray]]  # (seed) -> (X (DataFrame-like), y)\n",
    "\n",
    "\n",
    "def _onehot_dense():\n",
    "    \"\"\"\n",
    "    OneHotEncoder that yields dense output across sklearn versions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "\n",
    "def make_preprocessor_for_df(X):\n",
    "    \"\"\"\n",
    "    Builds a ColumnTransformer for numeric + categorical columns.\n",
    "    Outputs:\n",
    "      - sparse for baseline pipelines (fine)\n",
    "      - dense for IORFA (we'll set sparse_threshold=0.0 to force dense if possible)\n",
    "    \"\"\"\n",
    "    # Works for pandas DataFrame; if user passes numpy, treat all numeric.\n",
    "    try:\n",
    "        import pandas as pd  # noqa: F401\n",
    "        is_df = hasattr(X, \"dtypes\")\n",
    "    except Exception:\n",
    "        is_df = hasattr(X, \"dtypes\")\n",
    "\n",
    "    if not is_df:\n",
    "        # All numeric\n",
    "        numeric_pipe = Pipeline(\n",
    "            steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "        )\n",
    "        return ColumnTransformer([(\"num\", numeric_pipe, slice(0, X.shape[1]))], remainder=\"drop\")\n",
    "\n",
    "    num_cols = list(X.select_dtypes(include=[\"number\", \"bool\"]).columns)\n",
    "    cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "    numeric_pipe = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "    cat_pipe = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), (\"onehot\", _onehot_dense())]\n",
    "    )\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_pipe, num_cols),\n",
    "            (\"cat\", cat_pipe, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        sparse_threshold=0.0,  # try to keep output dense (helps IORFA)\n",
    "    )\n",
    "\n",
    "\n",
    "def subsample(X, y, max_n: Optional[int], seed: int):\n",
    "    if max_n is None:\n",
    "        return X, y\n",
    "    n = len(y)\n",
    "    if n <= max_n:\n",
    "        return X, y\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(n, size=max_n, replace=False)\n",
    "    if hasattr(X, \"iloc\"):\n",
    "        return X.iloc[idx], y[idx]\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "\n",
    "def metrics(y_true, y_pred) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"RMSE\": float(mean_squared_error(y_true, y_pred)),\n",
    "        \"MAE\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"R2\": float(r2_score(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "\n",
    "def make_datasets() -> List[DatasetSpec]:\n",
    "    def diabetes(seed: int):\n",
    "        d = load_diabetes()\n",
    "        X = d.data\n",
    "        y = d.target\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.permutation(len(y))\n",
    "        return X[idx], y[idx]\n",
    "\n",
    "    def california(seed: int):\n",
    "        d = fetch_california_housing()\n",
    "        X = d.data\n",
    "        y = d.target\n",
    "        rng = np.random.default_rng(seed)\n",
    "        idx = rng.permutation(len(y))\n",
    "        return X[idx], y[idx]\n",
    "\n",
    "    def openml_regression(data_id: int, target_col: Optional[str] = None):\n",
    "        def _loader(seed: int):\n",
    "            bunch = fetch_openml(data_id=data_id, as_frame=True)\n",
    "            X = bunch.data\n",
    "            y = bunch.target\n",
    "            # If multi-target, pick one column\n",
    "            if hasattr(y, \"shape\") and len(getattr(y, \"shape\", ())) == 2 and y.shape[1] > 1:\n",
    "                if target_col is not None and hasattr(y, \"__getitem__\"):\n",
    "                    yv = np.asarray(y[target_col], dtype=float).ravel()\n",
    "                else:\n",
    "                    yv = np.asarray(y.iloc[:, 0], dtype=float).ravel()\n",
    "            else:\n",
    "                yv = np.asarray(y, dtype=float).ravel()\n",
    "\n",
    "            rng = np.random.default_rng(seed)\n",
    "            idx = rng.permutation(len(yv))\n",
    "            return X.iloc[idx], yv[idx]\n",
    "        return _loader\n",
    "\n",
    "    return [\n",
    "        DatasetSpec(\"diabetes_sklearn\", diabetes),\n",
    "        DatasetSpec(\"california_housing_sklearn\", california),\n",
    "\n",
    "        # OpenML real regression datasets (by data_id)\n",
    "        DatasetSpec(\"abalone_183\", openml_regression(183)),\n",
    "        DatasetSpec(\"wine_quality_287\", openml_regression(287)),\n",
    "        DatasetSpec(\"kin8nm_189\", openml_regression(189)),\n",
    "        DatasetSpec(\"cpu_act_197\", openml_regression(197)),\n",
    "        DatasetSpec(\"cpu_small_562\", openml_regression(562)),\n",
    "        DatasetSpec(\"yacht_hydrodynamics_42370\", openml_regression(42370)),\n",
    "\n",
    "        # Energy Efficiency has 2 targets; pick one if present (names vary; fallback is first col)\n",
    "        DatasetSpec(\"energy_efficiency_1472\", openml_regression(1472, target_col=\"Y1\")),\n",
    "\n",
    "        DatasetSpec(\"concrete_strength_44959\", openml_regression(44959)),\n",
    "    ]\n",
    "\n",
    "\n",
    "def make_baselines(seed: int) -> Dict[str, object]:\n",
    "    return {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge(alpha=1.0)\": Ridge(alpha=1.0, random_state=seed),\n",
    "        \"Lasso(alpha=0.01)\": Lasso(alpha=0.01, max_iter=20000, random_state=seed),\n",
    "        \"CART\": DecisionTreeRegressor(random_state=seed),\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=300, random_state=seed, n_jobs=-1),\n",
    "        \"ExtraTrees\": ExtraTreesRegressor(n_estimators=400, random_state=seed, n_jobs=-1),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(random_state=seed),\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # ---- knobs ----\n",
    "    seed = 0\n",
    "    test_size = 0.25\n",
    "\n",
    "    # Keep datasets \"not too big\" in practice\n",
    "    max_n_all = 5000        # subsample for ALL models (set None to use full)\n",
    "    max_n_iorfa = 600       # subsample for IORFA specifically (MIQP grows fast)\n",
    "\n",
    "    # IORFA settings (safe-ish defaults)\n",
    "    iorfa_max_depth = 2\n",
    "    iorfa_alpha = 0.01\n",
    "    iorfa_timelimit = 60    # seconds per dataset\n",
    "    iorfa_output = False\n",
    "\n",
    "    datasets = make_datasets()\n",
    "    baselines = make_baselines(seed)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for ds in datasets:\n",
    "        print(f\"\\n=== {ds.name} ===\")\n",
    "        try:\n",
    "            X, y = ds.loader(seed)\n",
    "        except Exception as e:\n",
    "            print(f\"SKIP (load failed): {type(e).__name__}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # subsample globally (keeps “not too big”)\n",
    "        X, y = subsample(X, y, max_n_all, seed)\n",
    "\n",
    "        # split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=seed\n",
    "        )\n",
    "\n",
    "        # preprocessing\n",
    "        pre = make_preprocessor_for_df(X_train)\n",
    "\n",
    "        # ----- baselines -----\n",
    "        for name, model in baselines.items():\n",
    "            pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "            t0 = time.perf_counter()\n",
    "            pipe.fit(X_train, y_train)\n",
    "            fit_s = time.perf_counter() - t0\n",
    "            pred = pipe.predict(X_test)\n",
    "            m = metrics(y_test, pred)\n",
    "            rows.append({\n",
    "                \"dataset\": ds.name, \"model\": name,\n",
    "                \"n_train\": len(y_train), \"n_test\": len(y_test),\n",
    "                \"RMSE\": m[\"RMSE\"], \"MAE\": m[\"MAE\"], \"R2\": m[\"R2\"],\n",
    "                \"fit_seconds\": fit_s, \"iorfa_optgap\": None, \"iorfa_status\": None\n",
    "            })\n",
    "            print(f\"{name:16s} RMSE={m['RMSE']:.4f}  MAE={m['MAE']:.4f}  R2={m['R2']:.4f}  fit_s={fit_s:.2f}\")\n",
    "\n",
    "        # ----- IORFA -----\n",
    "        try:\n",
    "            Xi_train, yi_train = subsample(X_train, y_train, max_n_iorfa, seed)\n",
    "            Xi_test, yi_test = subsample(X_test, y_test, max_n_iorfa, seed + 1)\n",
    "\n",
    "            # Fit transformer on IORFA train and transform to dense numpy\n",
    "            pre_i = make_preprocessor_for_df(Xi_train)\n",
    "            Xi_train_p = pre_i.fit_transform(Xi_train)\n",
    "            Xi_test_p = pre_i.transform(Xi_test)\n",
    "\n",
    "            # choose min_samples_split defensively\n",
    "            mss = max(2, min(10, int(0.05 * len(yi_train))))\n",
    "\n",
    "            # dynamic gamma bounds (tends to help solver)\n",
    "            y_min, y_max = float(np.min(yi_train)), float(np.max(yi_train))\n",
    "            yr = max(1.0, y_max - y_min)\n",
    "            gamma_bounds = (y_min - yr, y_max + yr)\n",
    "\n",
    "            model = optimalDecisionTreeClassifier(\n",
    "                max_depth=iorfa_max_depth,\n",
    "                min_samples_split=mss,\n",
    "                alpha=iorfa_alpha,\n",
    "                warmstart=True,\n",
    "                timelimit=iorfa_timelimit,\n",
    "                output=iorfa_output,\n",
    "                gamma_bounds=gamma_bounds,\n",
    "            )\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            model.fit(Xi_train_p, yi_train)\n",
    "            fit_s = time.perf_counter() - t0\n",
    "            pred = model.predict(Xi_test_p)\n",
    "\n",
    "            m = metrics(yi_test, pred)\n",
    "            optgap = getattr(model, \"optgap\", None)\n",
    "            status = getattr(getattr(model, \"m\", None), \"status\", None)\n",
    "\n",
    "            rows.append({\n",
    "                \"dataset\": ds.name,\n",
    "                \"model\": f\"IORFA(d={iorfa_max_depth},a={iorfa_alpha})\",\n",
    "                \"n_train\": len(yi_train), \"n_test\": len(yi_test),\n",
    "                \"RMSE\": m[\"RMSE\"], \"MAE\": m[\"MAE\"], \"R2\": m[\"R2\"],\n",
    "                \"fit_seconds\": fit_s, \"iorfa_optgap\": optgap, \"iorfa_status\": status\n",
    "            })\n",
    "            print(f\"{'IORFA':16s} RMSE={m['RMSE']:.4f}  MAE={m['MAE']:.4f}  R2={m['R2']:.4f}  fit_s={fit_s:.2f}  gap={optgap}  status={status}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"IORFA failed: {type(e).__name__}: {e}\")\n",
    "            rows.append({\n",
    "                \"dataset\": ds.name,\n",
    "                \"model\": f\"IORFA(d={iorfa_max_depth},a={iorfa_alpha})\",\n",
    "                \"n_train\": None, \"n_test\": None,\n",
    "                \"RMSE\": None, \"MAE\": None, \"R2\": None,\n",
    "                \"fit_seconds\": None, \"iorfa_optgap\": None, \"iorfa_status\": None,\n",
    "                \"error\": f\"{type(e).__name__}: {e}\",\n",
    "            })\n",
    "\n",
    "    # ----- save results -----\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(\"benchmark_results_real.csv\", index=False)\n",
    "        print(\"\\nSaved: benchmark_results_real.csv\")\n",
    "        print(\"\\nTop rows (sorted by dataset then RMSE):\")\n",
    "        print(df.sort_values([\"dataset\", \"RMSE\"], na_position=\"last\").head(30).to_string(index=False))\n",
    "    except ImportError:\n",
    "        print(\"\\nInstall pandas to save CSV nicely: pip install pandas\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae50efc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "An-Optimal-RuleFit-Algorithm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
